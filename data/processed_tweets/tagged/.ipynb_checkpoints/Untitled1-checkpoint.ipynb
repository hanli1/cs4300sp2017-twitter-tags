{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('art_lover_processed.csv', header=None)\n",
    "t2 = pd.read_csv('conservative_processed.csv', header=None)\n",
    "t3 = pd.read_csv('fashion_lover_processed.csv', header=None)\n",
    "t4 = pd.read_csv('food_lover_processed.csv', header=None)\n",
    "t5 = pd.read_csv('gamer_processed.csv', header=None)\n",
    "t6 = pd.read_csv('liberal_processed.csv', header=None)\n",
    "t7 = pd.read_csv('music_lover_processed.csv', header=None)\n",
    "t8 = pd.read_csv('religious_processed.csv', header=None)\n",
    "t9 = pd.read_csv('science_lover_processed.csv', header=None)\n",
    "t10 = pd.read_csv('sports_fan_processed.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"art_lover_processed.csv\".find(\"_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1['CATEGORY'] = \"ART\"\n",
    "t2['CATEGORY'] = \"CONSERVATIVE\"\n",
    "t3['CATEGORY'] = \"FASHION\"\n",
    "t4['CATEGORY'] = \"FOOD\"\n",
    "t5['CATEGORY'] = \"GAMER\"\n",
    "t6['CATEGORY'] = \"LIBERAL\"\n",
    "t7['CATEGORY'] = \"MUSIC\"\n",
    "t8['CATEGORY'] = \"RELIGIOUS\"\n",
    "t9['CATEGORY'] = \"SCIENCE\"\n",
    "t10['CATEGORY'] = \"SPORTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [t1, t2, t3, t4, t5, t6, t7, t8, t9, t10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853594038699778048</td>\n",
       "      <td>Saatchi Art</td>\n",
       "      <td>2017-04-16 13:00:52</td>\n",
       "      <td>12</td>\n",
       "      <td>learn chang game arthistori</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>853504508604940291</td>\n",
       "      <td>Saatchi Art</td>\n",
       "      <td>2017-04-16 07:05:06</td>\n",
       "      <td>8</td>\n",
       "      <td>natalia ryabova see natalia' work artistoftheday</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853412730883239936</td>\n",
       "      <td>Saatchi Art</td>\n",
       "      <td>2017-04-16 01:00:25</td>\n",
       "      <td>15</td>\n",
       "      <td>littl goe long way</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853322176795209730</td>\n",
       "      <td>Saatchi Art</td>\n",
       "      <td>2017-04-15 19:00:35</td>\n",
       "      <td>9</td>\n",
       "      <td>meet theotherartfair' robin clare whose work e...</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>853232733363073024</td>\n",
       "      <td>Saatchi Art</td>\n",
       "      <td>2017-04-15 13:05:10</td>\n",
       "      <td>29</td>\n",
       "      <td>revisit sacr piec honor da vinci' 565th birthday</td>\n",
       "      <td>ART</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0            1                    2   3  \\\n",
       "0  853594038699778048  Saatchi Art  2017-04-16 13:00:52  12   \n",
       "1  853504508604940291  Saatchi Art  2017-04-16 07:05:06   8   \n",
       "2  853412730883239936  Saatchi Art  2017-04-16 01:00:25  15   \n",
       "3  853322176795209730  Saatchi Art  2017-04-15 19:00:35   9   \n",
       "4  853232733363073024  Saatchi Art  2017-04-15 13:05:10  29   \n",
       "\n",
       "                                                   4 CATEGORY  \n",
       "0                        learn chang game arthistori      ART  \n",
       "1   natalia ryabova see natalia' work artistoftheday      ART  \n",
       "2                                 littl goe long way      ART  \n",
       "3  meet theotherartfair' robin clare whose work e...      ART  \n",
       "4   revisit sacr piec honor da vinci' 565th birthday      ART  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.concat(frames, ignore_index = True)\n",
    "tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate_tweets(data):\n",
    "    text = \"\"\n",
    "    users = []\n",
    "    tweets = []\n",
    "    category = []\n",
    "    for index, row in data.iterrows():\n",
    "        if len(users) == 0:\n",
    "            users.append(row[1])\n",
    "            category.append(row[5])\n",
    "        elif users[-1] != row[1]:\n",
    "            users.append(row[1])\n",
    "            category.append(row[5])\n",
    "            tweets.append(text)\n",
    "            text = \"\"\n",
    "        text = text + row[4] + \" \" \n",
    "    tweets.append(text)\n",
    "    return pd.DataFrame({'User': users, 'Tweets': tweets, 'Category': category})\n",
    "\n",
    "agg_tweets = aggregate_tweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ART</td>\n",
       "      <td>learn chang game arthistori natalia ryabova se...</td>\n",
       "      <td>Saatchi Art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ART</td>\n",
       "      <td>brief histori happen impact art check 180 work...</td>\n",
       "      <td>Artsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ART</td>\n",
       "      <td>new spin old art form richli woven tapestri ad...</td>\n",
       "      <td>Art.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ART</td>\n",
       "      <td>8pm monday 10 april art monthli talk show paul...</td>\n",
       "      <td>Art Monthly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CONSERVATIVE</td>\n",
       "      <td>happi easter everyon may celebr fill joy love ...</td>\n",
       "      <td>GOP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CONSERVATIVE</td>\n",
       "      <td>divers though may christian around world unit ...</td>\n",
       "      <td>Paul Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CONSERVATIVE</td>\n",
       "      <td>american peopl need economi work happi easter ...</td>\n",
       "      <td>House Republicans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CONSERVATIVE</td>\n",
       "      <td>happi easter god gave begotten son might save ...</td>\n",
       "      <td>Sarah Palin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CONSERVATIVE</td>\n",
       "      <td>heidi wish famili happi easter congratul thank...</td>\n",
       "      <td>Ted Cruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FASHION</td>\n",
       "      <td>pvc trouser thump annagmurphi dare go straples...</td>\n",
       "      <td>Times Fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FASHION</td>\n",
       "      <td>hurri want miss shopbop' big event love shop f...</td>\n",
       "      <td>Who What Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FASHION</td>\n",
       "      <td>miranda kerr step fianc evan spiegel perfect d...</td>\n",
       "      <td>Vogue Magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FASHION</td>\n",
       "      <td>help karliekloss tax offer financi advic decad...</td>\n",
       "      <td>NYT Fashion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FASHION</td>\n",
       "      <td>happi easter throwback overs bunni ear marcjac...</td>\n",
       "      <td>FashionWeek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>today day final get weird brown stuff sheet pa...</td>\n",
       "      <td>Bon Appétit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>spici spring brunch option pork tenderloin alw...</td>\n",
       "      <td>NYT Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>secret tiki bar coachella coachella2017 can fi...</td>\n",
       "      <td>L.A. Times Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>let chicken marinad day quick flavor weeknight...</td>\n",
       "      <td>PBS Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>subscrib game informer' weekli podcast learn l...</td>\n",
       "      <td>Game Informer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>star war celebration' creativ cosplay reveal r...</td>\n",
       "      <td>GameSpot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>wish one happi easter beauti spring may joy gl...</td>\n",
       "      <td>Game Insight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>play shini mysteri block 'qube 2' microsoft pl...</td>\n",
       "      <td>Engadget Gaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>2 hour left sign week' big eu open miss chanc ...</td>\n",
       "      <td>Good Gaming, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>log gtaonlin day easter wknd amp unlock match ...</td>\n",
       "      <td>Rockstar Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>reliv adventur capcom' icon blue bomber mega m...</td>\n",
       "      <td>PlayStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>easter color just egg xboxdesignlab star war b...</td>\n",
       "      <td>Xbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>celebr weekend slope steep_gam level ghost' lo...</td>\n",
       "      <td>Ubisoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GAMER</td>\n",
       "      <td>get readi lost legaci revisit chloe' debut unc...</td>\n",
       "      <td>Naughty Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LIBERAL</td>\n",
       "      <td>believ state countri support trump' disastr ri...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LIBERAL</td>\n",
       "      <td>tomperez amp berniesand team go tour next week...</td>\n",
       "      <td>The Democrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LIBERAL</td>\n",
       "      <td>heart goe victim famili london act terror can ...</td>\n",
       "      <td>Barack Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LIBERAL</td>\n",
       "      <td>set nation thrill new york get done hope first...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>one listen abl stop listen jaysomband chanceth...</td>\n",
       "      <td>Pitchfork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>palm tree celeb come stream revolv desert hous...</td>\n",
       "      <td>Twitter Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>listen latest msldemarco onthelevel straight c...</td>\n",
       "      <td>Apple Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>send birthday wish jacque today don t want mis...</td>\n",
       "      <td>Universal MusicGroup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>alt pop rise star thatgirlbishop share 6 track...</td>\n",
       "      <td>Amazon Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>ladi gaga drop new track right coachella set l...</td>\n",
       "      <td>Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>watch harri styles' spot imperson mick jagger ...</td>\n",
       "      <td>billboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>happi birthday chancetherapp send good vibe ch...</td>\n",
       "      <td>Vevo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>tonight reliv classic beege hit stayin aliv gr...</td>\n",
       "      <td>Recording Academy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>t boz amp chilli back listen wayback ft snoopd...</td>\n",
       "      <td>Pandora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MUSIC</td>\n",
       "      <td>maggierog take starbuck atmospher today check ...</td>\n",
       "      <td>Spotify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RELIGIOUS</td>\n",
       "      <td>christian invit church easter sunday christian...</td>\n",
       "      <td>CNN Religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RELIGIOUS</td>\n",
       "      <td>enough simpli wish love compass grow within us...</td>\n",
       "      <td>Dalai Lama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RELIGIOUS</td>\n",
       "      <td>happi easter may bring joy hope risen christ t...</td>\n",
       "      <td>Pope Francis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RELIGIOUS</td>\n",
       "      <td>religion stylebook good friday commemor day je...</td>\n",
       "      <td>Religion News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RELIGIOUS</td>\n",
       "      <td>podcast easter stori rebirth tasmania s anglic...</td>\n",
       "      <td>ABC Religion&amp;Ethics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RELIGIOUS</td>\n",
       "      <td>san bernardino shooter s christian reveal huge...</td>\n",
       "      <td>HuffPost Religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>icymi cassinisaturn amp nasahubbl discov ingre...</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>first time scientist will abl watch western he...</td>\n",
       "      <td>Scientific American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>scisign genom evolut protein phosphatas everyt...</td>\n",
       "      <td>Science Magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>everyth goe well smartphon addict lead person ...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>trailer might give away happen thor hulk colli...</td>\n",
       "      <td>WIRED Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>legaci volcan erupt preserv gentoo penguin poo...</td>\n",
       "      <td>Science News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>smart home beginn five rad random thing found ...</td>\n",
       "      <td>Popular Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>200 nba playoff game king jame 21 michael jord...</td>\n",
       "      <td>ESPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>happi easter coke pepsi ok tfw iso joe start g...</td>\n",
       "      <td>Yahoo Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>watch creas stream live denver rocknighthawk v...</td>\n",
       "      <td>Twitter Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>day 1 nba playoff book learn joe johnson buri ...</td>\n",
       "      <td>FOX Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                             Tweets  \\\n",
       "0            ART  learn chang game arthistori natalia ryabova se...   \n",
       "1            ART  brief histori happen impact art check 180 work...   \n",
       "2            ART  new spin old art form richli woven tapestri ad...   \n",
       "3            ART  8pm monday 10 april art monthli talk show paul...   \n",
       "4   CONSERVATIVE  happi easter everyon may celebr fill joy love ...   \n",
       "5   CONSERVATIVE  divers though may christian around world unit ...   \n",
       "6   CONSERVATIVE  american peopl need economi work happi easter ...   \n",
       "7   CONSERVATIVE  happi easter god gave begotten son might save ...   \n",
       "8   CONSERVATIVE  heidi wish famili happi easter congratul thank...   \n",
       "9        FASHION  pvc trouser thump annagmurphi dare go straples...   \n",
       "10       FASHION  hurri want miss shopbop' big event love shop f...   \n",
       "11       FASHION  miranda kerr step fianc evan spiegel perfect d...   \n",
       "12       FASHION  help karliekloss tax offer financi advic decad...   \n",
       "13       FASHION  happi easter throwback overs bunni ear marcjac...   \n",
       "14          FOOD  today day final get weird brown stuff sheet pa...   \n",
       "15          FOOD  spici spring brunch option pork tenderloin alw...   \n",
       "16          FOOD  secret tiki bar coachella coachella2017 can fi...   \n",
       "17          FOOD  let chicken marinad day quick flavor weeknight...   \n",
       "18         GAMER  subscrib game informer' weekli podcast learn l...   \n",
       "19         GAMER  star war celebration' creativ cosplay reveal r...   \n",
       "20         GAMER  wish one happi easter beauti spring may joy gl...   \n",
       "21         GAMER  play shini mysteri block 'qube 2' microsoft pl...   \n",
       "22         GAMER  2 hour left sign week' big eu open miss chanc ...   \n",
       "23         GAMER  log gtaonlin day easter wknd amp unlock match ...   \n",
       "24         GAMER  reliv adventur capcom' icon blue bomber mega m...   \n",
       "25         GAMER  easter color just egg xboxdesignlab star war b...   \n",
       "26         GAMER  celebr weekend slope steep_gam level ghost' lo...   \n",
       "27         GAMER  get readi lost legaci revisit chloe' debut unc...   \n",
       "28       LIBERAL  believ state countri support trump' disastr ri...   \n",
       "29       LIBERAL  tomperez amp berniesand team go tour next week...   \n",
       "30       LIBERAL  heart goe victim famili london act terror can ...   \n",
       "31       LIBERAL  set nation thrill new york get done hope first...   \n",
       "32         MUSIC  one listen abl stop listen jaysomband chanceth...   \n",
       "33         MUSIC  palm tree celeb come stream revolv desert hous...   \n",
       "34         MUSIC  listen latest msldemarco onthelevel straight c...   \n",
       "35         MUSIC  send birthday wish jacque today don t want mis...   \n",
       "36         MUSIC  alt pop rise star thatgirlbishop share 6 track...   \n",
       "37         MUSIC  ladi gaga drop new track right coachella set l...   \n",
       "38         MUSIC  watch harri styles' spot imperson mick jagger ...   \n",
       "39         MUSIC  happi birthday chancetherapp send good vibe ch...   \n",
       "40         MUSIC  tonight reliv classic beege hit stayin aliv gr...   \n",
       "41         MUSIC  t boz amp chilli back listen wayback ft snoopd...   \n",
       "42         MUSIC  maggierog take starbuck atmospher today check ...   \n",
       "43     RELIGIOUS  christian invit church easter sunday christian...   \n",
       "44     RELIGIOUS  enough simpli wish love compass grow within us...   \n",
       "45     RELIGIOUS  happi easter may bring joy hope risen christ t...   \n",
       "46     RELIGIOUS  religion stylebook good friday commemor day je...   \n",
       "47     RELIGIOUS  podcast easter stori rebirth tasmania s anglic...   \n",
       "48     RELIGIOUS  san bernardino shooter s christian reveal huge...   \n",
       "49       SCIENCE  icymi cassinisaturn amp nasahubbl discov ingre...   \n",
       "50       SCIENCE  first time scientist will abl watch western he...   \n",
       "51       SCIENCE  scisign genom evolut protein phosphatas everyt...   \n",
       "52       SCIENCE  everyth goe well smartphon addict lead person ...   \n",
       "53       SCIENCE  trailer might give away happen thor hulk colli...   \n",
       "54       SCIENCE  legaci volcan erupt preserv gentoo penguin poo...   \n",
       "55       SCIENCE  smart home beginn five rad random thing found ...   \n",
       "56        SPORTS  200 nba playoff game king jame 21 michael jord...   \n",
       "57        SPORTS  happi easter coke pepsi ok tfw iso joe start g...   \n",
       "58        SPORTS  watch creas stream live denver rocknighthawk v...   \n",
       "59        SPORTS  day 1 nba playoff book learn joe johnson buri ...   \n",
       "\n",
       "                    User  \n",
       "0            Saatchi Art  \n",
       "1                  Artsy  \n",
       "2                Art.com  \n",
       "3            Art Monthly  \n",
       "4                    GOP  \n",
       "5              Paul Ryan  \n",
       "6      House Republicans  \n",
       "7            Sarah Palin  \n",
       "8               Ted Cruz  \n",
       "9          Times Fashion  \n",
       "10         Who What Wear  \n",
       "11        Vogue Magazine  \n",
       "12           NYT Fashion  \n",
       "13           FashionWeek  \n",
       "14           Bon Appétit  \n",
       "15              NYT Food  \n",
       "16       L.A. Times Food  \n",
       "17              PBS Food  \n",
       "18         Game Informer  \n",
       "19              GameSpot  \n",
       "20          Game Insight  \n",
       "21       Engadget Gaming  \n",
       "22     Good Gaming, Inc.  \n",
       "23        Rockstar Games  \n",
       "24           PlayStation  \n",
       "25                  Xbox  \n",
       "26               Ubisoft  \n",
       "27           Naughty Dog  \n",
       "28        Bernie Sanders  \n",
       "29         The Democrats  \n",
       "30          Barack Obama  \n",
       "31       Hillary Clinton  \n",
       "32             Pitchfork  \n",
       "33         Twitter Music  \n",
       "34           Apple Music  \n",
       "35  Universal MusicGroup  \n",
       "36          Amazon Music  \n",
       "37                Genius  \n",
       "38             billboard  \n",
       "39                  Vevo  \n",
       "40     Recording Academy  \n",
       "41               Pandora  \n",
       "42               Spotify  \n",
       "43          CNN Religion  \n",
       "44            Dalai Lama  \n",
       "45          Pope Francis  \n",
       "46         Religion News  \n",
       "47   ABC Religion&Ethics  \n",
       "48     HuffPost Religion  \n",
       "49                  NASA  \n",
       "50   Scientific American  \n",
       "51      Science Magazine  \n",
       "52               Science  \n",
       "53         WIRED Science  \n",
       "54          Science News  \n",
       "55       Popular Science  \n",
       "56                  ESPN  \n",
       "57          Yahoo Sports  \n",
       "58        Twitter Sports  \n",
       "59            FOX Sports  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweets_to_words(tweet):\n",
    "    tweet = re.sub(r\"http\\S+\", \" \", tweet)\n",
    "\n",
    "    text = BeautifulSoup(tweet).get_text() \n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "   \n",
    "    words = letters_only.lower().split()                             \n",
    "\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ltang/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/ltang/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n",
      "PROCESSSING\n"
     ]
    }
   ],
   "source": [
    "num_tweets = tweets[4].size\n",
    "clean_tweets = []\n",
    "for i in xrange( 0, num_tweets ):\n",
    "    clean_tweets.append(tweets_to_words(tweets[4][i]))\n",
    "    if i %10000 == 0:\n",
    "        print(\"PROCESSSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'icymi anything resounding iowa win loss clinton iacaucus', u'leadright', u'first votes happen today iowa find iacaucus location', u'ia caucuses today nh debate days nh primary days gopconvention days election days', u'despite decrying influx money politics clinton benefitting figure checks super pacs']\n",
      "12000    CONSERVATIVE\n",
      "12001    CONSERVATIVE\n",
      "12002    CONSERVATIVE\n",
      "12003    CONSERVATIVE\n",
      "12004    CONSERVATIVE\n",
      "Name: CATEGORY, dtype: category\n",
      "Categories (10, object): [ART, CONSERVATIVE, FASHION, FOOD, ..., MUSIC, RELIGIOUS, SCIENCE, SPORTS]\n"
     ]
    }
   ],
   "source": [
    "print clean_tweets[12000:12005]\n",
    "print tweets['CATEGORY'][12000:12005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 6000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweets, tweets['CATEGORY'], test_size=0.05, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(clean_tweets)\n",
    "train_data_features = features.toarray()\n",
    "\n",
    "test_data_features = vectorizer.transform(X_test)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_features = vectorizer.fit_transform(clean_tweets)\n",
    "X_test_features = vectorizer.fit_transform(X_test)\n",
    "\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_features)\n",
    "# X_test_tfidf = tfidf_transformer.transform(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100) \n",
    "# rf = rf.fit(X_train_tfidf, y_train)\n",
    "rf = rf.fit(train_data_features, tweets['CATEGORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.89     ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 1.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       ..., \n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         1.       ],\n",
       "       [ 0.62     ,  0.03     ,  0.0461039, ...,  0.01     ,  0.01     ,\n",
       "         0.09     ],\n",
       "       [ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = rf.predict_proba(test_data_features)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CONSERVATIVE', 'GAMER', 'ART', ..., 'SPORTS', 'ART', 'FOOD'], dtype=object)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = np.array(y_test)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017056467243829952"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.mean( pred != obs )\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweets, tweets['CATEGORY'], test_size=0.1, random_state=129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "llf = svm.LinearSVC()\n",
    "llf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_prob_nb = nb.predict_proba(X_test)\n",
    "pred_nb = nb.predict(X_test)\n",
    "\n",
    "pred_svm = llf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCIENCE' 'GAMER' 'MUSIC' 'LIBERAL' 'MUSIC' 'SCIENCE' 'SCIENCE' 'GAMER'\n",
      " 'MUSIC' 'ART']\n",
      "['SCIENCE' 'GAMER' 'MUSIC' 'CONSERVATIVE' 'MUSIC' 'SCIENCE' 'SCIENCE'\n",
      " 'GAMER' 'MUSIC' 'ART']\n"
     ]
    }
   ],
   "source": [
    "print pred_nb[10:20]\n",
    "print pred_svm[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858768718802\n",
      "0.888519134775\n"
     ]
    }
   ],
   "source": [
    "print np.mean(pred_nb == np.array(y_test))\n",
    "print np.mean(pred_svm == np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61794            GAMER\n",
       "107327           MUSIC\n",
       "67729            GAMER\n",
       "128994         SCIENCE\n",
       "86432            MUSIC\n",
       "28477          FASHION\n",
       "45424            GAMER\n",
       "131943         SCIENCE\n",
       "15303     CONSERVATIVE\n",
       "129840         SCIENCE\n",
       "Name: CATEGORY, dtype: object"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ART', u'CONSERVATIVE', u'FASHION', u'FOOD', u'GAMER', u'LIBERAL',\n",
       "       u'MUSIC', u'RELIGIOUS', u'SCIENCE', u'SPORTS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test, dtype=\"category\").cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82429285,  0.03447587,  0.06422629,  0.07700499])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM is right more often which makes sense\n",
    "table = [0,0,0,0]\n",
    "total = len(y_test)\n",
    "for i, cat in enumerate(np.array(y_test)):\n",
    "    if pred_nb[i] == cat and pred_svm[i] == cat:\n",
    "        table[0] += 1\n",
    "    elif pred_nb[i] == cat:\n",
    "        table[1] +=1\n",
    "    elif pred_svm[i] == cat:\n",
    "        table[2] += 1\n",
    "    else:\n",
    "        table[3] += 1\n",
    "np.divide(table, float(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i, row in enumerate(pred_prob[0:500]):\n",
    "#     if np.max(row) < 0.5:\n",
    "#         print np.array(X_test)[i]\n",
    "#         print \"nb: \" + pred_nb[i]\n",
    "#         print np.max(row)\n",
    "#         print \"svm: \" + pred_svm[i]\n",
    "#         print \"Actual: \" + np.array(y_test)[i]\n",
    "#         print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(len(tweets), n_folds = 10, shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy: 0.854301896112\n",
      "[0.85271214642262894, 0.85291181364392676, 0.85710482529118137, 0.85144758735440929, 0.8547753743760399, 0.85397670549084859, 0.85104825291181363, 0.84998336106489181, 0.85916805324459233, 0.85989084132055382]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for train_idx, test_idx in kf:\n",
    "    X_train = vectorizer.fit_transform(np.array(clean_tweets)[train_idx])\n",
    "    X_test = vectorizer.transform(np.array(clean_tweets)[test_idx])\n",
    "      \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "    pred = nb.predict(X_test)\n",
    "    accuracy.append(np.mean(pred == np.array(tweets['CATEGORY'][test_idx])))\n",
    "\n",
    "print(\"Avg Accuracy: \" + str(np.mean(accuracy)))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88851913477537436"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweets, tweets['CATEGORY'], test_size=0.1, random_state=129)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "llf = svm.LinearSVC()\n",
    "llf.fit(X_train, y_train)\n",
    "pred = llf.predict(X_test)\n",
    "np.mean(pred == np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How to make final model\n",
    "idx_to_cat = [u'ART', u'CONSERVATIVE', u'FASHION', u'FOOD', u'GAMER', u'LIBERAL',u'MUSIC', u'RELIGIOUS', u'SCIENCE', u'SPORTS']\n",
    "cat_to_idx = {u'ART':0,\n",
    "              u'CONSERVATIVE':1, \n",
    "              u'FASHION':2, \n",
    "              u'FOOD':3, \n",
    "              u'GAMER':4, \n",
    "              u'LIBERAL':5,\n",
    "              u'MUSIC':6,\n",
    "              u'RELIGIOUS':7, \n",
    "              u'SCIENCE':8, \n",
    "              u'SPORTS':9}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_tweets, tweets['CATEGORY'], test_size=0.1, random_state=129)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "llf = svm.LinearSVC()\n",
    "llf.fit(X_train, y_train)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "pred_nb_prob = nb.predict_proba(X_test)\n",
    "pred_nb = nb.predict(X_test)\n",
    "\n",
    "pred_svm = llf.predict(X_test)\n",
    "\n",
    "pred = pred_svm\n",
    "pred_prob = pred_nb_prob\n",
    "for i, row in enumerate(pred_nb_prob):\n",
    "    if np.max(row) >= 0.95:\n",
    "        pred[i] = pred_nb[i]\n",
    "    else:\n",
    "        svm_prob = [0]*10;\n",
    "        svm_prob[cat_to_idx[pred_svm[i]]] = 1\n",
    "        pred_prob[i] = np.divide(np.add(row, svm_prob),2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(200, 300):\n",
    "#     print np.array(y_test)[i]\n",
    "#     print pred[i]\n",
    "#     print idx_to_cat[np.argmax(pred_prob[i])]\n",
    "#     print np.max(pred_prob[i])\n",
    "#     print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8901164725457571"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred == np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pd.Series(y_test, dtype=\"category\").cat.categories\n",
    "# for i, row in enumerate(pred_prob[0:500]):\n",
    "#     if np.max(row) < 0.95:\n",
    "#         print np.array(X_test)[i]\n",
    "#         print \"nb: \" + pred_nb[i]\n",
    "#         print row\n",
    "#         print \"svm: \" + pred_svm[i]\n",
    "#         print \"Actual: \" + np.array(y_test)[i]\n",
    "#         print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88851913477537436"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = llf.predict(X_test)\n",
    "np.mean(pred == np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9635043 ,  0.0364957 ,  0.96045552,  0.03954448])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#At what point does NB make less correct choices than svm?\n",
    "table = [0,0,0,0]\n",
    "total = 0\n",
    "for i, cat in enumerate(np.array(y_test)):\n",
    "    if np.max(pred_prob_nb[i]) > 0.9:\n",
    "        if pred_nb[i] == cat:\n",
    "            table[0] += 1\n",
    "        else: \n",
    "            table[1] += 1\n",
    "        \n",
    "        if pred_svm[i] == cat:\n",
    "            table[2] += 1\n",
    "        else:\n",
    "            table[3] += 1\n",
    "        \n",
    "        total += 1\n",
    "np.divide(table, float(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(len(tweets), n_folds = 10, shuffle=True)    \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy: 0.886421863897\n",
      "[0.88805324459234614, 0.88698835274542431, 0.88985024958402659, 0.88918469217970053, 0.88399334442595678, 0.88665557404326123, 0.88039933444259566, 0.88532445923460901, 0.88871880199667219, 0.88505058572949946]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for train_idx, test_idx in kf:\n",
    "    X_train = vectorizer.fit_transform(np.array(clean_tweets)[train_idx])\n",
    "    X_test = vectorizer.transform(np.array(clean_tweets)[test_idx])\n",
    "      \n",
    "    llf = svm.LinearSVC()\n",
    "    llf.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "    pred_svm = llf.predict(X_test)\n",
    "    accuracy.append(np.mean(pred == np.array(tweets['CATEGORY'][test_idx])))\n",
    "\n",
    "print(\"Avg Accuracy: \" + str(np.mean(accuracy)))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(len(tweets), n_folds = 10, shuffle=True)    \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ART', u'CONSERVATIVE', u'FASHION', u'FOOD', u'GAMER', u'LIBERAL',\n",
       "       u'MUSIC', u'RELIGIOUS', u'SCIENCE', u'SPORTS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test, dtype=\"category\").cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy: 0.886574952732\n",
      "[0.88865224625623962, 0.8920465890183028, 0.88326123128119804, 0.88625623960066557, 0.88173044925124788, 0.88425956738768718, 0.88712146422628957, 0.88525790349417632, 0.89051580698835275, 0.8866480298189563]\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for train_idx, test_idx in kf:\n",
    "    X_train = vectorizer.fit_transform(np.array(clean_tweets)[train_idx])\n",
    "    X_test = vectorizer.transform(np.array(clean_tweets)[test_idx])\n",
    "      \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "    \n",
    "    llf = svm.LinearSVC()\n",
    "    llf.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "\n",
    "    pred_nb = nb.predict(X_test)\n",
    "    pred_nb_prob = nb.predict_proba(X_test)\n",
    "    \n",
    "    pred_svm = llf.predict(X_test)\n",
    "    \n",
    "    pred = pred_svm\n",
    "    for i, row in enumerate(pred_nb_prob):\n",
    "        if np.max(row) >= 0.9:\n",
    "            pred[i] = pred_nb[i]\n",
    "            \n",
    "    accuracy.append(np.mean(pred == np.array(tweets['CATEGORY'][test_idx])))\n",
    "\n",
    "print(\"Avg Accuracy: \" + str(np.mean(accuracy)))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy: 0.887060788374\n",
      "[0.88525790349417632, 0.88246256239600662, 0.88831946755407654, 0.88811980033277871, 0.88725457570715471, 0.89331114808652246, 0.88266222961730445, 0.89364392678868554, 0.88612312811980032, 0.88345314164004263]\n"
     ]
    }
   ],
   "source": [
    "#For now final model\n",
    "accuracy = []\n",
    "for train_idx, test_idx in kf:\n",
    "    X_train = vectorizer.fit_transform(np.array(clean_tweets)[train_idx])\n",
    "    X_test = vectorizer.transform(np.array(clean_tweets)[test_idx])\n",
    "      \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "    \n",
    "    llf = svm.LinearSVC()\n",
    "    llf.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "\n",
    "    pred_nb = nb.predict(X_test)\n",
    "    pred_nb_prob = nb.predict_proba(X_test)\n",
    "    \n",
    "    pred_svm = llf.predict(X_test)\n",
    "    \n",
    "    pred = pred_svm\n",
    "    for i, row in enumerate(pred_nb_prob):\n",
    "        if np.max(row) >= 0.95:\n",
    "            pred[i] = pred_nb[i]\n",
    "            \n",
    "    accuracy.append(np.mean(pred == np.array(tweets['CATEGORY'][test_idx])))\n",
    "\n",
    "print(\"Avg Accuracy: \" + str(np.mean(accuracy)))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy: 0.887992570955\n",
      "[0.88519134775374375, 0.88452579034941758, 0.88905158069883528, 0.88905158069883528, 0.88745424292845254, 0.89450915141430953, 0.88379367720465896, 0.89437603993344428, 0.88818635607321128, 0.88378594249201281]\n"
     ]
    }
   ],
   "source": [
    "idx_to_cat = [u'ART', u'CONSERVATIVE', u'FASHION', u'FOOD', u'GAMER', u'LIBERAL',u'MUSIC', u'RELIGIOUS', u'SCIENCE', u'SPORTS']\n",
    "cat_to_idx = {u'ART':0,\n",
    "              u'CONSERVATIVE':1, \n",
    "              u'FASHION':2, \n",
    "              u'FOOD':3, \n",
    "              u'GAMER':4, \n",
    "              u'LIBERAL':5,\n",
    "              u'MUSIC':6,\n",
    "              u'RELIGIOUS':7, \n",
    "              u'SCIENCE':8, \n",
    "              u'SPORTS':9}\n",
    "\n",
    "#Testing to see if average probability weight actually works better\n",
    "accuracy = []\n",
    "for train_idx, test_idx in kf:\n",
    "    X_train = vectorizer.fit_transform(np.array(tweets[4])[train_idx])\n",
    "    X_test = vectorizer.transform(np.array(tweets[4])[test_idx])\n",
    "      \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "    \n",
    "    llf = svm.LinearSVC()\n",
    "    llf.fit(X_train, tweets['CATEGORY'][train_idx])\n",
    "\n",
    "    pred_nb = nb.predict(X_test)\n",
    "    pred_nb_prob = nb.predict_proba(X_test)\n",
    "    \n",
    "    pred_svm = llf.predict(X_test)\n",
    "            \n",
    "    pred = pred_svm\n",
    "    pred_prob = pred_nb_prob\n",
    "    for i, row in enumerate(pred_nb_prob):\n",
    "        if np.max(row) >= 0.95:\n",
    "            pred[i] = pred_nb[i]\n",
    "        else:\n",
    "            svm_prob = [0]*10;\n",
    "            svm_prob[cat_to_idx[pred_svm[i]]] = 1\n",
    "            pred_prob[i] = np.add(np.multiply(0.5, row), np.multiply(0.5,svm_prob))\n",
    "            pred[i] = idx_to_cat[np.argmax(pred_prob[i])]\n",
    "            \n",
    "    accuracy.append(np.mean(pred == np.array(tweets['CATEGORY'][test_idx])))\n",
    "\n",
    "print(\"Avg Accuracy: \" + str(np.mean(accuracy)))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  1. ,  1.5,  2. ])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = [1,2,3,4]\n",
    "np.multiply(0.5, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-448-fd8ff4b2ae88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mn_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mmap_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     def _limit_features(self, X, vocabulary, high=None, low=None,\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m        \u001b[0;31m# [1:2,[1,2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# row is slice, col is sequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;31m# [[1,2],??]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    489\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m            indptr)\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mnnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = KFold(len(agg_tweets), n_folds = 10, shuffle=True)    \n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)\n",
    "accuracy = []\n",
    "for train_idx, test_idx in kf:\n",
    "    X_train = vectorizer.fit_transform(np.array(agg_tweets['Tweets'])[train_idx])\n",
    "    X_test = vectorizer.transform(np.array(agg_tweets['Tweets'])[test_idx])\n",
    "      \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, agg_tweets['Category'][train_idx])\n",
    "    \n",
    "    llf = svm.LinearSVC()\n",
    "    llf.fit(X_train, agg_tweets['Category'][train_idx])\n",
    "\n",
    "    pred_nb = nb.predict(X_test)\n",
    "    pred_nb_prob = nb.predict_proba(X_test)\n",
    "    \n",
    "    pred_svm = llf.predict(X_test)\n",
    "    \n",
    "    pred = pred_svm\n",
    "    for i, row in enumerate(pred_nb_prob):\n",
    "        if np.max(row) >= 0.95:\n",
    "            pred[i] = pred_nb[i]\n",
    "            \n",
    "    accuracy.append(np.mean(pred == np.array(agg_tweets['Category'][test_idx])))\n",
    "    break;\n",
    "\n",
    "print(\"Avg Accuracy: \" + str(np.mean(accuracy)))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
